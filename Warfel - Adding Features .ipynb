{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math \n",
      "#import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import random\n",
      "#import prettyplotlib as ppl\n",
      "#from sklearn import ensemble as ske\n",
      "%matplotlib inline\n",
      "#import seaborn as sns\n",
      "import re\n",
      "from django.utils.encoding import smart_str, smart_unicode\n",
      "\n",
      "            \n",
      "df = pd.io.json.read_json('data/train.json')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\pytz\\__init__.py:29: UserWarning: Module argparse was already imported from C:\\Python27\\lib\\argparse.pyc, but c:\\python27\\lib\\site-packages is being added to sys.path\n",
        "  from pkg_resources import resource_stream\n",
        "C:\\Python27\\lib\\site-packages\\pandas\\io\\excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use >=1.6.1 and <2.0.0.\n",
        "  .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##cleaning some weird non ascii characters\n",
      "df['clean_request_text'] = df['request_text_edit_aware'].apply(lambda x: smart_str(x)) \n",
      "#df.drop('request_text_edit_aware',1)\n",
      "df['clean_title_text'] = df['request_title'].apply(lambda x: smart_str(x))\n",
      "#df.drop('request_title')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FEATURES ADDED: Length of posts  titles, log transformed\n",
      "df['request_length'] = df['clean_request_text'] .apply(lambda x: len(x))\n",
      "df['log_request_length'] = df['request_length'].apply(lambda x: math.log10(x+1))\n",
      "\n",
      "#Avoiding Collinearity in predictors\n",
      "df = df.drop('request_length',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FEATURES ADDED: Length of requests, log transformed\n",
      "df['request_title_length'] = df['request_title'].apply(lambda x:len(x))\n",
      "df['log_title_length'] = df['request_title_length'].apply(lambda x: math.log10(x+1))\n",
      "\n",
      "df = df.drop('request_title_length',1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FEATURE ADDED: presence of link\n",
      "def link_test(inputstring):\n",
      "    x = inputstring\n",
      "    a = re.search(\".com\", x)\n",
      "    b = re.search(\"http\", x)\n",
      "    c = re.search(\".net\",x)\n",
      "    d = a or b or c\n",
      "    if d:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "                                                \n",
      "df['link_presence?'] = df['clean_request_text'].apply(lambda j: link_test(j))\n",
      "                              "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FEATURE ADDED: presence of semi-colon, as a rough proxy for level of education, and well-written-ness\n",
      "def colon_test(inputstring):\n",
      "    x = inputstring\n",
      "    a = re.search(\";\", x)\n",
      "    if a:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "    \n",
      "df['semi_colon?'] = df['clean_request_text'].apply(lambda j: colon_test(j))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#FEATURE ADDED: Length of Username (longer ones tend to be more witty), and giver if known\n",
      "\n",
      "df['request_user_name_length'] = df['requester_username'].apply(lambda j: len(j))\n",
      "\n",
      "df['giver_user_name_length'] = df['giver_username_if_known'].apply(lambda j:len(j))\n",
      "                                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### hacking the topical narratives from the paper:\n",
      "#could supplement these...\n",
      "\n",
      "###Open question: the paper only used a binary count. maybe we should add that too? also, maybe we should consider verbosity by also \n",
      "#diving by length of text\n",
      "\n",
      "money1 = \"\"\" week ramen paycheck work couple rice check pizza grocery rent anyone favor someone bill money\"\"\"\n",
      "money2 = \"\"\"  food money house bill rent stamp month today parent help pizza someone anything mom anyone\"\"\"\n",
      "job =  \"\"\"job month rent year interview bill luck school pizza paycheck unemployment money ramen end check\"\"\"\n",
      "friend = \"\"\" friend house night mine pizza birthday thing school site place family story way movie anything\"\"\"\n",
      "student= \"\"\" student college final pizza loan summer university money class meal year semester story kid school\"\"\"\n",
      "familytime = \"\"\" tonight night today tomorrow someone anyone friday dinner something account family bank anything home work\"\"\"\n",
      "time = \"\"\" day couple anything today work pizza help pay anyone home meal food ramen someone favor\"\"\"\n",
      "gratitude = \"\"\" thanks advance guy reading anyone pizza  anything story tonight help place everyone craving kind favor\"\"\"\n",
      "pizza = \"\"\"pizza craving hut story someone anyone domino money cheese thing request picture act title kind\"\"\"\n",
      "general = \"\"\"time pizza year people part work hour life thing lurker story anything someone month way\"\"\"\n",
      "\n",
      "narratives = [money1,money2,job,friend,student,familytime,time,gratitude,pizza,general]\n",
      "### to do: create  10 columns for each request, each column with the number of points scored\n",
      "narrative_names = '''money1 money2 job friend student familytime time gratitude pizza general'''\n",
      "\n",
      "def narrative_scorer(text):\n",
      "    narrative_scores = []\n",
      "    names = narrative_names.split()\n",
      "    \n",
      "    for i in range(len(narratives)):\n",
      "        current_keywords = narratives[i].split()\n",
      "        narrative_score = 0\n",
      "\n",
      "        for word in current_keywords: ###This doesnt work yet\n",
      "            a = re.search(word,text) ####>>>>>?????\n",
      "           # print a\n",
      "            if a:\n",
      "                narrative_score += 1\n",
      "            else:\n",
      "                pass\n",
      "        narrative_scores.append(narrative_score)\n",
      "        \n",
      "    return narrative_scores\n",
      "\n",
      "df['clean_request_text'][0:5].apply(lambda x: narrative_scorer(x))\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "0    [0, 2, 0, 2, 0, 2, 2, 1, 0, 1]\n",
        "1    [1, 2, 1, 0, 1, 1, 2, 0, 1, 0]\n",
        "2    [2, 2, 3, 2, 2, 0, 1, 2, 3, 1]\n",
        "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "4    [1, 1, 3, 6, 2, 1, 1, 4, 2, 4]\n",
        "Name: clean_request_text, dtype: object"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "narrativedf = pd.DataFrame(df['clean_request_text'].apply(lambda x: narrative_scorer(x)))\n",
      "narrativedf.head()\n",
      "\n",
      "####TODO - Join this dataframe with df \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>clean_request_text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> [0, 2, 0, 2, 0, 2, 2, 1, 0, 1]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> [1, 2, 1, 0, 1, 1, 2, 0, 1, 0]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> [2, 2, 3, 2, 2, 0, 1, 2, 3, 1]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> [1, 1, 3, 6, 2, 1, 1, 4, 2, 4]</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "               clean_request_text\n",
        "0  [0, 2, 0, 2, 0, 2, 2, 1, 0, 1]\n",
        "1  [1, 2, 1, 0, 1, 1, 2, 0, 1, 0]\n",
        "2  [2, 2, 3, 2, 2, 0, 1, 2, 3, 1]\n",
        "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "4  [1, 1, 3, 6, 2, 1, 1, 4, 2, 4]"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#####################\n",
      "#LDA on titles#\n",
      "\n",
      "#Still need to run model backwords on new 'docbow' instances\n",
      "#Importing gensim\n",
      "import gensim #####\n",
      "#import logging\n",
      "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "from gensim import corpora, models, similarities\n",
      "import codecs\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#need to transfrom text from data frame into a list of strings. (for both request text and request title? or should they merged?\n",
      "#keeping them seperate for now\n",
      "request = df['request_text_edit_aware']\n",
      "Reqtxt = []\n",
      "for item in request:\n",
      "     Reqtxt.append(smart_str(item))\n",
      "title= df['request_title']\n",
      "Titletxt = []\n",
      "for item in title:\n",
      "    Titletxt.append(smart_str(item))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##next version: update stoplist \n",
      "stoplist = set(\"\"\"for a of the and me my to in is i I it am be has [request] [Request] [REQUEST] request (request) pizza pizza! pizza, pizza.\n",
      "               have would this i'm on when but as just that get with reddit subreddit if at so no are or\n",
      "               we was out can not some someone\"\"\".split())\n",
      "Rtexts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "        for document in Reqtxt]\n",
      "Ttexts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "        for document in Titletxt]\n",
      "\n",
      "###NNNEED TO REMOVE ALL WORDS THAT AREN\"T NOUNS\n",
      "\n",
      "###option: remove all words that only appear once in the entire dataset. [potentially train LDA on Test and Train???]\n",
      "\n",
      "# remove words that appear only once\n",
      "#Rall_tokens = sum(Rtexts, [])\n",
      "#Rtokens_once = set(word for word in set(Rall_tokens) if Rall_tokens.count(word) == 1)\n",
      "#Rtexts = [[word for word in text if word not in Rtokens_once]\n",
      "         #for text in Rtexts]\n",
      "\n",
      "#Tall_tokens = sum(Ttexts, [])\n",
      "#Ttokens_once = set(word for word in set(Tall_tokens) if Tall_tokens.count(word) == 1)\n",
      "#Ttexts = [[word for word in text if word not in Ttokens_once]\n",
      "      #   for text in Ttexts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Rdict = corpora.Dictionary(Rtexts)\n",
      "Tdict = corpora.Dictionary(Ttexts)           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Rdict.save('request_dictionary.dict')\n",
      "Tdict.save('title_dictionary.dict')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##creating a corpus\n",
      "Rcorpus = [Rdict.doc2bow(text) for text in Rtexts]\n",
      "Tcorpus = [Tdict.doc2bow(text) for text in Ttexts]\n",
      "corpora.MmCorpus.serialize('Rcorpus.mm', Rcorpus)\n",
      "corpora.MmCorpus.serialize('Tcorpus.mm', Tcorpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Rlda_23 = gensim.models.ldamodel.LdaModel(corpus=Rcorpus, id2word=Rdict, num_topics=23, update_every=20, chunksize=100, passes=1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Rlda_23.save('Rlda_23.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Rlda_23.print_topics(23)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 149,
       "text": [
        "[u'0.021*). + 0.010*been + 0.006*could + 0.006*help + 0.006*sense + 0.005*really + 0.004*you + 0.004*week + 0.004*our + 0.004*even',\n",
        " u'0.020*looking! + 0.015*been + 0.010*noodle + 0.010*honest, + 0.010*loan + 0.010*past, + 0.010*troubles + 0.010*jimmy + 0.010*eyes, + 0.009*:)',\n",
        " u\"0.018*all + 0.012*food + 0.009*we're + 0.009*remembered + 0.009*arrived + 0.008*cheers! + 0.008*egift + 0.008*roomate + 0.008*cant + 0.008*florida\",\n",
        " u\"0.012*you + 0.009*until + 0.008*i've + 0.008*friday + 0.008*food + 0.007*don't + 0.007*day + 0.007*today + 0.007*help + 0.007*next\",\n",
        " u\"0.011*you + 0.009*until + 0.009*about + 0.009*been + 0.007*help + 0.007*will + 0.006*food + 0.006*also + 0.006*money + 0.006*i've\",\n",
        " u'0.015*alive + 0.014*searched + 0.010*emotionally + 0.010*cabin, + 0.010*bed + 0.009*stressed + 0.009*ground. + 0.009*wonders + 0.006*pay + 0.006*tell',\n",
        " u'0.014*food + 0.011*our + 0.009*he + 0.009*his + 0.008*hunger + 0.008*tend + 0.008*do + 0.008*all + 0.007*been + 0.006*date',\n",
        " u\"0.013** + 0.011*addition + 0.011*georgia. + 0.011*that'd + 0.011*15th. + 0.010*money + 0.010*hell. + 0.010*fair + 0.009*help + 0.008*day\",\n",
        " u'0.020*forever. + 0.019*bugging + 0.019*sticky + 0.018*epic + 0.017*times. + 0.015*friend. + 0.012*&lt;3 + 0.011*law + 0.011*couch + 0.009*will',\n",
        " u'0.019*kids + 0.018*her + 0.012*you + 0.011*she + 0.010*tried + 0.008*our + 0.007*thank + 0.007*pepperoni + 0.007*mother, + 0.007*attempt',\n",
        " u'0.013*him + 0.012*his + 0.011*he + 0.008*pay + 0.008*got + 0.007*been + 0.007*cake + 0.007*soo + 0.007*quickly + 0.007*cleaning',\n",
        " u\"0.020*homework, + 0.020*broccoli + 0.015*woo! + 0.013*games + 0.011*:) + 0.008*there's + 0.008*pie + 0.007*edit: + 0.007*pizza](http://bigepizza.com)! + 0.007*hotspot\",\n",
        " u'0.017*all + 0.014*you + 0.013*food + 0.011*now + 0.010*:) + 0.009*want + 0.009*return + 0.009*love + 0.008*up + 0.008*$15',\n",
        " u\"0.016*been + 0.010*help + 0.008*had + 0.008*i've + 0.008*you + 0.007*able + 0.007*food + 0.007*there + 0.007*really + 0.007*after\",\n",
        " u'0.020*this) + 0.014*proud + 0.011*belly + 0.011*&lt;-- + 0.011*swell. + 0.011*mad + 0.011*two. + 0.011*p.s. + 0.011*pancreatic + 0.011*etc.,',\n",
        " u'0.040*foster + 0.020*voice + 0.020*(of + 0.019*halfway + 0.015*meat. + 0.009*live + 0.008*food + 0.006*sos + 0.006*creek + 0.006*coconut',\n",
        " u'0.019*atlanta, + 0.019*time! + 0.019*promotion + 0.019*deviantart + 0.019*ga. + 0.017*upcoming + 0.013*link + 0.012*commissions + 0.012*ao + 0.012*you',\n",
        " u\"0.021*er + 0.012*favor, + 0.011*deciding + 0.011*virtually + 0.009*i'll + 0.009*dish + 0.008*sense + 0.008*laptop + 0.008*weird + 0.008*you\",\n",
        " u\"0.025*hook + 0.016*up + 0.014*month's + 0.014*shipping + 0.014*papa + 0.014*johns + 0.011*tomorrow. + 0.011*could + 0.011*gift + 0.010*here\",\n",
        " u\"0.020*little + 0.012*stories + 0.012*today! + 0.012*boyfriend's + 0.012*calls + 0.012*i'd + 0.012*trick. + 0.012*jack + 0.012*else's + 0.010*fulfill\",\n",
        " u'0.017*morning, + 0.017*me; + 0.015*chest + 0.014*chances + 0.012*alcohol + 0.007*bills, + 0.006*been + 0.006*pay + 0.005*pizza?! + 0.005*she',\n",
        " u\"0.012*grandmother + 0.011*after + 0.008*it's + 0.008*i've + 0.008*there + 0.008*about + 0.007*home + 0.007*possible. + 0.007*fix + 0.006*you\",\n",
        " u\"0.017*help + 0.008*i've + 0.007*you + 0.007*can't + 0.007*friend + 0.007*any + 0.006*thanks + 0.006*out. + 0.006*screen + 0.006*kind\"]"
       ]
      }
     ],
     "prompt_number": 149
    }
   ],
   "metadata": {}
  }
 ]
}